Linear Regression is a form of modeling in Data Science where predictors, or independent variables, are graphed with the response of a target, or dependent variable. Linear Regressions can take on different forms, depending on how many predictors there are, how many target variables, or how correlated predictors and target variables may be to themselves or each other. For example, when there is more than one predictor feature, we describe this as a multiple linear regression. When multiple target features are correlated with predictor features we have a multivariate linear regression, since multiple variables influence the outcome of our target.

Linear Regression is considered a predictive form of modeling, since the goal of a linear regression is to make estimates on future values. For example, a linear regression of housing data may be used to predict future selling points. This is in contrast with other data science models which seek to classify data, rather than predict it. Linear Regression isn’t simply the predicted values, however. It is also a measure of variance between independent and dependent variables; how one may vary with the other. Responses can be high, or incredibly low. We gauge the effect of responses in a linear regression to determine which independent variables may have the strongest effect on our dependent variables; and which may have none.

Linear Regression sounds convoluted at first, but it follows the formula y = mx+b where m is the slope, x is our independent variable, and b is the y-intercept. A multivariate linear regression expands upon this form by adding various slopes and independent variables with a y-intercept.

Linear Regression comes with some assumptions, if it is to be performed correctly. First, the data used in a Linear Regression must be normally distributed; that its, data must fall within three standard deviations of a mean,  or these values will be considered outliers. The second assumption is that all values are linearly related with the target variable. For example, if we were looking at the square footage of a home, we would assume that there would be a linear relationship between the square footage and the price of the home; price, naturally, increasing as square footage increases. The third assumption is homoscedasticity. When a dataset is considered homoscedastic, the variance of error between the independent variables and the dependent variables is the same across all values of the model. This is important because if the errors of a model are not homoscedastic, and are spread unevenly, then certain features could be creating an unequal weight on our predictor variables. This would skew our model, and present us with bad predictions.

To ensure data that is normalized, linearly related, and homoscedastic, it may be necessary to perform to perform min-max scaling, or log transformations. These mathematical formulas establish an equal scale between variables in a dataset, and ensure they are normally distributed. From here, we can then create stratified samples for our training and test sets. Using k-fold validation, and categorical encoding, we can create sets that are representative of our data as a whole, and variable enough to teach our Linear Regression model to adapt to data we will present it with later with our test group.

Once we have plotted our linear regression, we can see our predictor values alongside our predicted values. There will always be some measure of error. We can visualize these errors using a bar graph, showing the discrepancy between our expected values, and the actual values seen in our dataset. These margins of error can then be graphed with a graph of our residuals.

But these graphs only show us the errors present in our model, they do not seek to correct them. To generate better predictions, we can use what’s called “ensemble learning.” Here, we use different models, such as Decision Tree Regressors, Random Forest Regressors, polynomial regression, or even grid searching to find the features that influence our model the most. After these models have been made, we can calculate the margin of error between them.

We can then create an OLS regression to measure, among other things, the level of certainty we have with our model. In an OLS regression, we have multiple components to interpret. For example, the Adjusted R-Squared values range from 0 to 1, where a higher value means our model is better fit to our data. Const coefficient is the y-intercept of the model.  The lower the Std err, the greater the the level of accuracy of our coefficient (slope) of the model.  The confidence interval represents the range the slope of the model is expected to stretch. P>\t\ represents our p-value, or the level of statistical significance in our model; anything 0.05 or lower means our results are statistically significant.  

So, with all these features, models, and measures in place, we can say that at the 0.05 level of certainty, our results represent a statistically significant relation between our predictor and target values, that allows us to accurately measure future values.
